{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST1348.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNh6xofeGkFzq+cV7yYF4AJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrasoonPratham/Kaggle/blob/main/MNIST1348.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp5jynV5CL64"
      },
      "source": [
        "# My solution for the MNIST dataset for the digits 1,3,4 and 8 challenge on kaggle.\n",
        "\n",
        "\n",
        "---\n",
        "![MNIST Dataset](https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/MnistExamples.png/440px-MnistExamples.png)\n",
        "\n",
        "MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n",
        "\n",
        "This is a Digit Recognization multiclass classfication problem. The data set is a subset on MNSIT dataset, only containing images of 1, 4,3 and 8\n",
        "\n",
        "Source : [Kaggle](https://www.kaggle.com/c/digitrecognizer1438)\n",
        "\n",
        "\n",
        "> In this notebook I will attempt to make a machine learning model for this subset of the MNIST dataset.\n",
        "\n",
        "You can follow me on Twitter : [@PrasoonPratham](https://twitter.com/PrasoonPratham)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x19mYztd3gzW"
      },
      "source": [
        "#Using the Kaggle API to fetch the dataset for this competition\n",
        "\n",
        "!pip install kaggle\n",
        "api_token = {\"username\":\"prathamprasoon\",\"key\":\"4eec6bdf09208b0e38d6f39fcec31bde\"}\n",
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "!mkdir /root/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!echo '{\"username\":\"USERNAME\",\"key\":\"API_KEY\"}' > /root/.kaggle/kaggle.json\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "!kaggle competitions download -c digitrecognizer1438"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFoA0BXi3vx_"
      },
      "source": [
        "#Renaming files as certain issues are caused with the default ones\n",
        "%%bash\n",
        "mv /content/trainData.csv.zip /content/traincsvData.zip\n",
        "mv /content/testData.csv.zip /content/testcsv.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxtNacAF4MQK"
      },
      "source": [
        "#Unzipping training data\n",
        "zip_ref = zipfile.ZipFile('/content/traincsvData.zip', 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0hTbfjE4WV_"
      },
      "source": [
        "#Unzipping test data\n",
        "zip_ref = zipfile.ZipFile('/content/testcsv.zip', 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAVL0ZU14ckW"
      },
      "source": [
        "import pandas as pd\n",
        "test_data = pd.read_csv('/content/testData.csv')\n",
        "train_data = pd.read_csv('/content/trainData.csv')\n",
        "train_labels = pd.read_csv('/content/trainLabels.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m16zPXZa492A"
      },
      "source": [
        "#Importing numpy and tensorflow\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzisU8115CRD"
      },
      "source": [
        "X_train = train_data\n",
        "Y_train = train_labels"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyu3mcAx5Jvd"
      },
      "source": [
        "#Normalizing data for better performance\n",
        "X_train = X_train/255.0\n",
        "test_data = test_data/255.0\n",
        "\n",
        "#Reshaping Data\n",
        "X_train = X_train.values.reshape(-1,28,28,1)\n",
        "test_data = test_data.values.reshape(-1,28,28,1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6WD2p2q5P5C"
      },
      "source": [
        "#Splitting training data for training and validation \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X1_train,Y1_train,X2_train,Y2_train=train_test_split(X_train,Y_train,test_size = 0.05, random_state=2)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvbTYiub5S6b"
      },
      "source": [
        "#Model architecture\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),                      \n",
        "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dveBLFJX5cPs"
      },
      "source": [
        "print(\"Fit model on training data\")\n",
        "history = model.fit(\n",
        "    X1_train,\n",
        "    X2_train,\n",
        "    batch_size=16,\n",
        "    epochs=15,\n",
        "    validation_data=(Y1_train, Y2_train),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NtJQsn35nwl"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc=history.history['accuracy']\n",
        "val_acc=history.history['val_accuracy']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
        "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.figure()\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
        "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "# Desired output. Charts with training and validation metrics. No crash :)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ev2hSej7dg3"
      },
      "source": [
        "predicted_classes = model.predict_classes(test_data)\n",
        "submissions=pd.DataFrame({\"ImageId\": list(range(1,len(predicted_classes)+1)),\n",
        "                         \"Label\": predicted_classes})\n",
        "submissions.to_csv(\"submission.csv\", index=False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}